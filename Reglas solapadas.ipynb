{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b713dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f97ac60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga tu dataset aquí\n",
    "df = pd.read_csv('./overlap_test_data.csv', sep=';')\n",
    "\n",
    "# Elimina las columnas que no son features: contienen 'timestamp' en el nombre\n",
    "df = df[[col for col in df.columns if 'timestamp' not in col]]\n",
    "\n",
    "#cambiar nombre de columnas paraque sea más característico\n",
    "df.rename(columns={\n",
    "    'attachment_B1': 'attachment_A2',\n",
    "    'checked_D1': 'status_label_B1',\n",
    "    'aux_C1': 'attachment_A1'\n",
    "}, inplace=True)\n",
    "# Reordenar las columnas\n",
    "df = df[['attachment_A1', 'attachment_A2', 'status_label_B1', 'variant']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1bafbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attachment_A1</th>\n",
       "      <th>attachment_A2</th>\n",
       "      <th>status_label_B1</th>\n",
       "      <th>variant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attachment_A1  attachment_A2  status_label_B1  variant\n",
       "0              1              1                1        1\n",
       "1              1              0                1        1\n",
       "2              0              0                0        2\n",
       "3              1              0                0        2\n",
       "4              0              1                0        3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2124dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "\n",
    "columna_objetivo = 'variant'\n",
    "min_samples_split = 1\n",
    "merge_ratio_e = 0.5\n",
    "\n",
    "####################3\n",
    "\n",
    "#básicamente crea la estructura de diccionario para almacenar las reglas y las reglas solapadas.\n",
    "def check_dict_structure(paths_dict, dict_key):\n",
    "    if dict_key not in paths_dict['paths']:\n",
    "        paths_dict['paths'][dict_key] = {'rules': [], 'overlapped_rules': []}\n",
    "    else:\n",
    "        if 'overlapped_rules' not in paths_dict['paths'][dict_key] :\n",
    "            paths_dict['paths'][dict_key]['overlapped_rules'] = []\n",
    "    return paths_dict\n",
    "\n",
    "#Esta función extrae los caminos desde la raíz hasta las hojas de un árbol de decisión\n",
    "def extract_paths(tree, feature_names):\n",
    "    paths = {'paths': {}, 'metadata': {}}\n",
    "    n_nodes = tree.tree_.node_count\n",
    "    children_left = tree.tree_.children_left\n",
    "    children_right = tree.tree_.children_right\n",
    "    feature = tree.tree_.feature\n",
    "    threshold = tree.tree_.threshold\n",
    "    value = tree.tree_.value\n",
    "\n",
    "    # Función para recorrer el árbol\n",
    "    def recurse(node, path):\n",
    "        if children_left[node] != children_right[node]:\n",
    "            # Si no es una hoja, sigue recorriendo\n",
    "            left_path = path + [f\"{feature_names[feature[node]]} <= {threshold[node]:.2f}\"]\n",
    "            right_path = path + [f\"{feature_names[feature[node]]} > {threshold[node]:.2f}\"]\n",
    "            recurse(children_left[node], left_path)\n",
    "            recurse(children_right[node], right_path)\n",
    "        else:\n",
    "            # Es una hoja, determina la clase y guarda el camino\n",
    "            class_index = np.argmax(value[node])\n",
    "            class_label = int(tree.classes_[class_index])\n",
    "            if class_label not in paths['paths']:\n",
    "                paths['paths'][class_label] = {'rules': [], 'overlapped_rules': []}\n",
    "            paths['paths'][class_label]['rules'].append(\" and \".join(path))\n",
    "\n",
    "    # Inicia el recorrido desde la raíz\n",
    "    recurse(0, [])\n",
    "\n",
    "    # Imprime los caminos para cada clase\n",
    "    # for label, paths_aux in paths['paths'].items():\n",
    "    #     print(f\"Clase {label}:\")\n",
    "    #     for path in paths_aux['rules']:\n",
    "    #         print(f\"  - {path}\")\n",
    "            \n",
    "    return paths\n",
    "\n",
    "\n",
    "\n",
    "# Función para obtener las reglas del árbol de decisión\n",
    "def get_decision_path(tree, X_sample):\n",
    "    feature_names = X_sample.columns\n",
    "    node_indicator = tree.decision_path(X_sample)\n",
    "    leaf_id = tree.apply(X_sample)\n",
    "    paths = []\n",
    "    for sample_id, node_index in enumerate(leaf_id):\n",
    "        path_nodes = node_indicator.indices[node_indicator.indptr[sample_id]:node_indicator.indptr[sample_id + 1]]\n",
    "        path = []\n",
    "        for node_id in path_nodes:\n",
    "            if tree.tree_.children_left[node_id] == tree.tree_.children_right[node_id]:  # Es una hoja\n",
    "                path.append(f\"class: {tree.classes_[np.argmax(tree.tree_.value[node_id])]}\")\n",
    "                break\n",
    "            else:\n",
    "                if X_sample.iloc[sample_id, tree.tree_.feature[node_id]] <= tree.tree_.threshold[node_id]:\n",
    "                    path.append(f\"{feature_names[tree.tree_.feature[node_id]]} <= {tree.tree_.threshold[node_id]:.2f}\")\n",
    "                else:\n",
    "                    path.append(f\"{feature_names[tree.tree_.feature[node_id]]} > {tree.tree_.threshold[node_id]:.2f}\")\n",
    "        # drop the last item from paths because it's the class\n",
    "        # path_class = path.pop(-1)\n",
    "        path.pop(-1)\n",
    "        # paths.append(\"(\" + \" and \".join(path) + \") -> \" + path_class)\n",
    "        \n",
    "        paths.append(\" and \".join(path))\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8134d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      " Tree Accuracy: 88.89% \n",
      "\n",
      "\n",
      "===================================\n",
      "Variants with 100% certainty:\n",
      "===================================\n",
      "  Classes whose guards have a 100% certainty: {3}\n",
      "  Class 3:\n",
      "    - (certainity 100%) attachment_A2 > 0.50 and status_label_B1 <= 0.50 \n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "Variants with uncertainty:\n",
      "===================================\n",
      "  Overlapping rules: the class 2 (ground_truth) is missclassified as the class 1 (prediction)\n",
      "\n",
      "  Class 2:\n",
      "    - (precision 100%)  attachment_A2 <= 0.50 and attachment_A1 <= 0.50\n",
      "    - (precision 100%)  attachment_A2 <= 0.50 and attachment_A1 > 0.50 and status_label_B1 <= 0.50\n",
      "    - (overlapped)      attachment_A2 <= 0.50 and attachment_A1 > 0.50 and status_label_B1 > 0.50\n",
      "  Class 1:\n",
      "    - (precision 100%)  attachment_A2 > 0.50 and status_label_B1 > 0.50\n",
      "    - (overlapped)      attachment_A2 <= 0.50 and attachment_A1 > 0.50 and status_label_B1 > 0.50\n"
     ]
    }
   ],
   "source": [
    "# Carga tu dataset aquí\n",
    "# df = pd.read_csv('tu_dataset.csv')\n",
    "X = df.drop(columna_objetivo, axis=1)\n",
    "y = df[columna_objetivo]\n",
    "\n",
    "# Divide el dataset en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# Entrenar el modelo de árbol de decisión\n",
    "tree = DecisionTreeClassifier()  # Usando entropía para simular C4.5\n",
    "tree.fit(X_train, y_train)\n",
    "########################################################1-6\n",
    "# Evaluar el modelo\n",
    "y_pred = tree.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'\\n===================================\\n Tree Accuracy: {accuracy * 100:.2f}% ')\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "paths_dict = extract_paths(tree, feature_names)\n",
    "\n",
    "certain_labels = set(pd.unique(y_test))\n",
    "\n",
    "if accuracy == 1.0:\n",
    "    tree_text = export_text(tree, feature_names=list(X.columns))\n",
    "    with open('./ficheros_resultado/full_tree.txt', 'w') as f:\n",
    "        f.write(tree_text)\n",
    "else:\n",
    "    tree_text = export_text(tree, feature_names=list(X.columns))\n",
    "    with open('./ficheros_resultado/failed_tree.txt', 'w') as f:\n",
    "        f.write(tree_text)\n",
    "    misclassified = y_test != y_pred\n",
    "    misclassified_data = X_test[misclassified]\n",
    "    misclassified_labels = y_test[misclassified]\n",
    "    misclassified_preds = y_pred[misclassified]\n",
    "    #i=0\n",
    "    #aquí se crean los conjuntos con los individuos mal clasificados\n",
    "    for label in pd.unique(misclassified_labels):\n",
    "        subset = misclassified_data[misclassified_labels == label]\n",
    "        subset_target = misclassified_labels[misclassified_labels == label]\n",
    "        subset_preds = misclassified_preds[misclassified_labels == label]\n",
    "        #######################################################\n",
    "        #######################################################\n",
    "        #######################################################\n",
    "        # Calcula la frecuencia del ítem con mayor frecuencia\n",
    "        freqs = Counter(subset_preds)\n",
    "        t_prima, max_freq = freqs.most_common(1)[0]\n",
    "        freq_t_prima = max_freq / len(subset_preds)\n",
    "        \n",
    "        if subset.empty:\n",
    "            continue\n",
    "        \n",
    "        # Verifica las condiciones y actualiza paths_dict\n",
    "        if len(subset_preds) >= min_samples_split and freq_t_prima > merge_ratio_e:      \n",
    "            # Obtener las reglas de decisión para las instancias misclassified\n",
    "            subset_paths = get_decision_path(tree, subset)\n",
    "            \n",
    "            # Guarda el subset con predicciones, etiquetas reales y caminos de decisión\n",
    "            subset['ground_truth'] = subset_target\n",
    "            subset['prediction'] = subset_preds\n",
    "            subset['decision_path'] = subset_paths\n",
    "            subset.to_csv(f'./ficheros_resultado/subset_for_class_{label}.csv', index=False)\n",
    "                         \n",
    "            t_prima_subset = subset[subset_preds == t_prima]\n",
    "            first_occurence = t_prima_subset.iloc[0]\n",
    "       \n",
    "            paths_dict = check_dict_structure(paths_dict, first_occurence['ground_truth'])\n",
    "            paths_dict['paths'][first_occurence['ground_truth']]['overlapped_rules'].append(first_occurence['decision_path'])\n",
    "            \n",
    "            paths_dict = check_dict_structure(paths_dict, first_occurence['prediction'])\n",
    "            paths_dict['paths'][first_occurence['prediction']]['rules'].remove(first_occurence['decision_path'])\n",
    "            paths_dict['paths'][first_occurence['prediction']]['overlapped_rules'].append(first_occurence['decision_path'])\n",
    "\n",
    "            \n",
    "            # obtener como una lista de tuplas, los pares distintos de valores que existen entre la columna ground_truth y prediction\n",
    "            x_missclassified_as_y = subset[['ground_truth', 'prediction']].drop_duplicates().values.tolist()\n",
    "            \n",
    "            for aux_pair in x_missclassified_as_y:\n",
    "                for un_label in aux_pair:\n",
    "                    if un_label in certain_labels:\n",
    "                        certain_labels.remove(un_label)\n",
    "              \n",
    "            \n",
    "            # Entrena un nuevo árbol con el subset\n",
    "            sub_tree = DecisionTreeClassifier(criterion='entropy')\n",
    "            sub_tree.fit(subset.drop(['prediction', 'ground_truth', 'decision_path'], axis=1), subset_target)\n",
    "            sub_tree_text = export_text(sub_tree, feature_names=list(subset.columns[:-3]))\n",
    "            \n",
    "            #dot_data = export_graphviz(sub_tree, out_file=None, feature_names=X.columns, class_names=np.unique(y).astype(str), filled=True)\n",
    "            #graph = graphviz.Source(dot_data)\n",
    "            #graph.view(filename=f\"./ficheros_resultado/sub_tree{i}\", directory='', cleanup=True)\n",
    "            #i+=1\n",
    "            paths_dict['metadata'] = {  \n",
    "                'guard': first_occurence['decision_path'],\n",
    "                'x_missclassified_as_y': x_missclassified_as_y,\n",
    "                'subset_indexes': t_prima_subset.index.tolist(),\n",
    "                'subset_size': len(subset_preds),\n",
    "                'fraction_over_subset': freq_t_prima\n",
    "            }\n",
    "            \n",
    "            with open(f'./ficheros_resultado/tree_for_class_{label}.txt', 'w') as f:\n",
    "                f.write(sub_tree_text)\n",
    "            \n",
    "\n",
    "print(f\"\\n\\n===================================\\nVariants with 100% certainty:\\n===================================\")\n",
    "print(f\"  Classes whose guards have a 100% certainty: {certain_labels}\")\n",
    "# Imprimir las rules asociadas a las clases que tienen una certeza del 100%\n",
    "for label in certain_labels:\n",
    "    print(f\"  Class {label}:\")\n",
    "    for rule in paths_dict['paths'][label]['rules']:\n",
    "        print(f\"    - (certainity 100%) {rule} \\n\")\n",
    "\n",
    "\n",
    "print(f\"\\n\\n===================================\\nVariants with uncertainty:\\n===================================\")\n",
    "if 'metadata' in paths_dict:\n",
    "    # Comprueba que exista la clave 'x_missclassified_ñas_y' en paths_dict['metadata']\n",
    "    if 'x_missclassified_as_y' in paths_dict['metadata']:\n",
    "        for confusion in paths_dict['metadata']['x_missclassified_as_y']:\n",
    "            print(f\"  Overlapping rules: the class {confusion[0]} (ground_truth) is missclassified as the class {confusion[1]} (prediction)\\n\")\n",
    "            # Imprimir las rules de las claves en 'paths' en paths_dict que estan contenidas en confusion\n",
    "            for key in confusion:\n",
    "\n",
    "                if key in paths_dict['paths']:\n",
    "                    print(f\"  Class {key}:\")\n",
    "                    for rule in paths_dict['paths'][key]['rules']:\n",
    "                            print(f\"    - (precision 100%)  {rule}\")\n",
    "                        \n",
    "                    if 'overlapped_rules' in paths_dict['paths'][key]:\n",
    "                        for rule in paths_dict['paths'][key]['overlapped_rules']:\n",
    "                            print(f\"    - (overlapped)      {rule}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "# Guarda las reglas de decisión en un archivo JSON\n",
    "with open('./ficheros_resultado/paths.json', 'w') as f:\n",
    "    json.dump(paths_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e02cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124bf52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
